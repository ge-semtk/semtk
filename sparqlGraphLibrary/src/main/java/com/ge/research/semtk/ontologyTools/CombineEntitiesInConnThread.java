package com.ge.research.semtk.ontologyTools;

import java.util.AbstractCollection;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Hashtable;

import com.ge.research.semtk.auth.HeaderTable;
import com.ge.research.semtk.auth.ThreadAuthenticator;
import com.ge.research.semtk.belmont.AutoGeneratedQueryTypes;
import com.ge.research.semtk.belmont.Node;
import com.ge.research.semtk.belmont.NodeDeletionTypes;
import com.ge.research.semtk.belmont.NodeGroup;
import com.ge.research.semtk.belmont.ValueConstraint;
import com.ge.research.semtk.edc.JobTracker;
import com.ge.research.semtk.edc.client.ResultsClient;
import com.ge.research.semtk.resultSet.Table;
import com.ge.research.semtk.sparqlToXLib.SparqlToXLibUtil;
import com.ge.research.semtk.sparqlX.SparqlConnection;
import com.ge.research.semtk.sparqlX.SparqlToXUtils;
import com.ge.research.semtk.utility.LocalLogger;

/**
 * Process a table full of requests to combine entities by lookup
 * @author 200001934
 *
 */
public class CombineEntitiesInConnThread extends Thread {
	public static String SAME_AS_CLASS_URI = "http://research.ge.com/semtk/EntityResolution#SameAs";
	public static String TARGET_PROP_URI = "http://research.ge.com/semtk/EntityResolution#target";
	public static String DUPLICATE_PROP_URI = "http://research.ge.com/semtk/EntityResolution#duplicate";
	
	private static int BATCH_SIZE = 2000;
	private static int ERROR_LIMIT = 100;
	
	private JobTracker tracker;
	private ResultsClient resultsClient;
	private String jobId;
	private OntologyInfo oInfo;
	private SparqlConnection conn;
	private String sameAsTypeUri;
	private String targetPropUri;
	private String duplicatePropUri;
	private ArrayList<String> deletePredicatesFromTarget;
	private ArrayList<String> deletePredicatesFromDuplicate;
	private Table errorTab;
	private HeaderTable headerTable = null;
	private RestrictionChecker checker = null;


	/** 
	 * "Normal" preferred use.  Other one might be deprecated soon.
	 * @param tracker
	 * @param resultsClient
	 * @param jobId
	 * @param oInfo
	 * @param conn
	 * @throws Exception
	 */
	public CombineEntitiesInConnThread(JobTracker tracker, ResultsClient resultsClient, String jobId,
			OntologyInfo oInfo, SparqlConnection conn) throws Exception {
		this(
				tracker,
				resultsClient,
				jobId,
				oInfo,
				conn,
				SAME_AS_CLASS_URI,
				TARGET_PROP_URI,
				DUPLICATE_PROP_URI,
				null,
				null);
	}
	/**
	 * Create a thread for combining entities by querying instances of sameAsTypeUri
	 * which has properties targetPropUri and duplicatePropUri
	 * 
	 * Prechecks - but not perfectly, as things like loops are tricky to detect beforehand
	 * Combines
	 * Deletes instances of sameAsTypeUri
	 * 
	 * @param tracker
	 * @param resultsClient
	 * @param jobId
	 * @param oInfo
	 * @param conn
	 * @param sameAsTypeUri
	 * @param targetPropUri
	 * @param duplicatePropUri
	 * @param deletePredicatesFromTarget - list of predicates to delete from target before merging
	 * @param deletePredicatesFromDuplicate - list of predicates to delete from duplicate 
	 * @throws Exception - catch all
	 */
	public CombineEntitiesInConnThread(JobTracker tracker, ResultsClient resultsClient, String jobId, 
			OntologyInfo oInfo, SparqlConnection conn,
			String sameAsTypeUri,
			String targetPropUri,
			String duplicatePropUri, 
			ArrayList<String> deletePredicatesFromTarget,
			ArrayList<String> deletePredicatesFromDuplicate) throws SemtkUserException, Exception {
		this.tracker = tracker;
		this.resultsClient = resultsClient;
		this.jobId = jobId;
		this.tracker.createJob(this.jobId);
		this.oInfo = oInfo;
		this.conn = conn;
		this.sameAsTypeUri = sameAsTypeUri;
		this.targetPropUri = targetPropUri;
		this.duplicatePropUri = duplicatePropUri;
		this.headerTable = ThreadAuthenticator.getThreadHeaderTable();
		this.checker = new RestrictionChecker(conn, oInfo);

		this.errorTab = new Table(new String [] { "Error Description" });
		
		CombineEntitiesWorker.replacePropertyAbbrev(this.deletePredicatesFromTarget);
		CombineEntitiesWorker.replacePropertyAbbrev(this.deletePredicatesFromDuplicate);
		
		// Check the properties in the table to make sure they're legal
		if (this.oInfo.getClass(this.sameAsTypeUri) == null) {
			throw new SemtkUserException("Can't find class in model: " + this.sameAsTypeUri);
		}
		if (this.oInfo.getProperty(this.targetPropUri) == null && !this.duplicatePropUri.equals(SparqlToXLibUtil.TYPE_PROP)) {
			throw new SemtkUserException("Can't find target property in model: " + this.targetPropUri);
		}
		
		if (this.oInfo.getProperty(this.duplicatePropUri) == null && !this.duplicatePropUri.equals(SparqlToXLibUtil.TYPE_PROP)) {
			throw new SemtkUserException("Can't find duplicate property in model: " + this.duplicatePropUri);
		}

	}
	
	
	public void run() {
		ThreadAuthenticator.authenticateThisThread(this.headerTable);
		
		try {
			// query retrieves SameAs that are eligible for next batch
			this.tracker.createJob(this.jobId);
			
			this.runPrecheck();
			
			// stop now if any errors have occurred
			if (this.errorTab.getNumRows() > 0) {
				this.resultsClient.execStoreTableResults(this.jobId, this.errorTab);
				this.tracker.setJobFailure(this.jobId, "Errors during pre-check.  No merging was attempted.");
				return;
			}

			
		} catch (Exception e) {
			LocalLogger.logToStdErr("Internal Error during precheck");
			LocalLogger.printStackTrace(e);
		}
		
		
		try {
			String nextBatchQuery = this.generateGetNextSameAsBatchSparql(BATCH_SIZE);
			int numSameAs = this.countSameAs();
			ArrayList<String> sameAsUriList = new ArrayList<String>();
			int recordsProcessed = 0;
			Table batchTable = null;
			
			HashSet<Triple> insertTriples = new HashSet<Triple>();
			HashSet<Triple> deleteTriples = new HashSet<Triple>();
			int TRIPLE_BATCH = 50;
			
			// preform the combining in batches so that order is correct
			do {
				batchTable = this.conn.getDefaultQueryInterface().executeQueryToTable(nextBatchQuery);
				
				// do the work
				for (int i=0; i < batchTable.getNumRows(); i++) {
					
					CombineEntitiesWorker w =  new CombineEntitiesWorker(
							this.oInfo, this.conn, 
							batchTable.getCell(i, "target"), batchTable.getCell(i, "duplicate"),
							batchTable.getCell(i, "target_types").split(" "), batchTable.getCell(i, "duplicate_types").split(" "), 
							this.deletePredicatesFromTarget, this.deletePredicatesFromDuplicate,
							checker
							);
					w.generateCombineTriples();
					insertTriples.addAll(w.getInsertTriples());
					deleteTriples.addAll(w.getDeleteTriples());
					sameAsUriList.add(batchTable.getCell(i, "same_as"));
					recordsProcessed += 1;
					
					
					if (insertTriples.size() > TRIPLE_BATCH || deleteTriples.size() > TRIPLE_BATCH) {
						// perform queries when there are enough triples
						this.runQueriesAndClearInputs(insertTriples, deleteTriples, sameAsUriList);
						
						int percent = Math.min(99, 50 + 50 * recordsProcessed / numSameAs);
						this.tracker.setJobPercentComplete(this.jobId, percent);
					}
				}
				
				// perform queries for left overs
				
				this.runQueriesAndClearInputs(insertTriples, deleteTriples, sameAsUriList);
					
				
			} while (batchTable.getNumRows() > 0);

			if (recordsProcessed == numSameAs) {
				
				if (recordsProcessed > 0) 
					this.tracker.setJobSuccess(this.jobId, String.format("Combined %d pairs of entities.", recordsProcessed));
				else {
					this.errorTab.addRow(new String [] {"No SameAs instances found."});
					this.resultsClient.execStoreTableResults(this.jobId, this.errorTab);
					this.tracker.setJobFailure(this.jobId, "Nothing to combine.");
				}
			
			} else {
				
				// query leftover SameAs and fail
				Table t = this.conn.getDefaultQueryInterface().executeQueryToTable(
						this.generateGetAllSameAsSparql(ERROR_LIMIT));
				if (t.getNumRows() != 0) {
					for (int i=0; i < t.getNumRows(); i++) {
						this.errorTab.addRow(new String [] {
								String.format("Remains after merging: %s", t.getCell(i, "same_as"))
							});
					}
				}
				this.resultsClient.execStoreTableResults(this.jobId, this.errorTab);
				this.tracker.setJobFailure(this.jobId, "Invalid SameAs instances (possibly loops) remain after merging");
				
			}

		} catch (Exception e) {
			LocalLogger.printStackTrace(e);
			try {
				// Exceptions are reported via the async job status mechanism
				this.errorTab.addRow(new String [] {"Internal error: " + e.toString()});
				this.resultsClient.execStoreTableResults(this.jobId, this.errorTab);
				this.tracker.setJobFailure(this.jobId, "Error during merge.  Incomplete merge occurred. \n" + e.getMessage());

			} catch (Exception ee) {
				// Bail from total mess with log messages and a stranded job
				LocalLogger.logToStdErr("CombineEntitiesTableThread stranded a job due to another exception during setJobFailure()");
				LocalLogger.printStackTrace(ee);
			}
		}
	}

	private void runQueriesAndClearInputs(AbstractCollection<Triple> insertTriples, AbstractCollection<Triple> deleteTriples, AbstractCollection<String> sameAsUriList) throws Exception {
		this.conn.getDefaultQueryInterface().executeQueryAndConfirm(
				SparqlToXLibUtil.generateInsertTriples(this.conn, insertTriples)
				);
		this.conn.getDefaultQueryInterface().executeQueryAndConfirm(
				SparqlToXLibUtil.generateDeleteTriples(this.conn, deleteTriples)
				);
		this.conn.getDefaultQueryInterface().executeQueryAndConfirm(
				SparqlToXLibUtil.generateDeleteUris(this.conn, sameAsUriList)
				);
		insertTriples.clear();
		deleteTriples.clear();
		sameAsUriList.clear();		
	}
	/**
	 * Run all pre-checking. 
	 * Group pre-checks:
	 * 		no object can be duplicate to two different primaries
	 *      no sameAs can have target==duplicate
	 *      cardinality of duplicate and target is exactly 1
	 * Individual pre-checks:
	 * 	    defined by CombineEntitiesWorker
	 * 
	 * Note that some kinds problems that should only result in left-over SameAs are not caught:
	 *       loops
	 *       
	 * Puts errors into this.errorTab
	 * @throws Exception - 
	 */
	private void runPrecheck() throws Exception {
		Table t;
		
		// precheck no object is duplicate to 2 different sameAs
		t = this.conn.getDefaultQueryInterface().executeQueryToTable(
				this.generateGetBadSameAsDuplicates(ERROR_LIMIT));
		if (t.getNumRows() != 0) {
			for (int i=0; i < t.getNumRows(); i++) {
				this.errorTab.addRow(new String [] {
						String.format("Object is duplicate to multiple targets: %s", t.getCell(i, "duplicate"))
					});
			}
		}
		this.tracker.setJobPercentComplete(this.jobId, 1 / 8);
		
		// precheck no object is target==duplicate
		t = this.conn.getDefaultQueryInterface().executeQueryToTable(
				this.generateGetBadSameAsDupEqTarget(ERROR_LIMIT));
		if (t.getNumRows() != 0) {
			for (int i=0; i < t.getNumRows(); i++) {
				this.errorTab.addRow(new String [] {
						String.format("SameAs object has target==duplicate. sameAs: %s object: %s", t.getCell(i, "same_as1"), t.getCell(i, "object"))
				});
			}
		}
		this.tracker.setJobPercentComplete(this.jobId, 2 / 8);
		
		// precheck no missing or multiple target or duplicate
		t = this.conn.getDefaultQueryInterface().executeQueryToTable(
				this.generateGetBadSameCardinalityErr(ERROR_LIMIT));
		if (t.getNumRows() != 0) {
			for (int i=0; i < t.getNumRows(); i++) {
				this.errorTab.addRow(new String [] {
						String.format("SameAs does not have exactly 1 target and 1 duplicate: %s", t.getCell(i, "same_as"))
				});
			}
		}
		this.tracker.setJobPercentComplete(this.jobId, 3 / 8);
		
	    // run preCheck BATCH_SIZE at a time
		int offset = 0;
		do {
			// check type
			t = this.conn.getDefaultQueryInterface().executeQueryToTable(this.generateGetCombiningDiffTypes(BATCH_SIZE, offset));
		
			for (int i=0; i < t.getNumRows(); i++) {
				CombineEntitiesWorker worker = new CombineEntitiesWorker(oInfo, conn, 
						t.getCell(i, "target"), t.getCell(i, "duplicate"), 
						this.deletePredicatesFromTarget, this.deletePredicatesFromDuplicate,
						checker);
				try {
					worker.preCheck();
				} catch (SemtkUserException e) {
					this.errorTab.addRow(new String [] {e.getMessage()});
				}
			}
			offset += BATCH_SIZE;
		} while (t.getNumRows() == BATCH_SIZE);
		
		this.tracker.setJobPercentComplete(this.jobId, 4 / 8);
	}
	
	private void setJobPercentComplete(int percent) {
		try {
			this.tracker.setJobPercentComplete(this.jobId, percent);
		} catch (Exception e) {}
	}
	
	/**
	 * Count the number of this.sameAsTypeUri instances in the 
	 * @return
	 * @throws Exception
	 */
	private int countSameAs() throws Exception {
		NodeGroup ng = new NodeGroup();
		ng.setSparqlConnection(this.conn);
		
		Node n = ng.addNode(this.sameAsTypeUri, this.oInfo);
		n.setIsReturned(true);
		String sparql = ng.generateSparql(AutoGeneratedQueryTypes.COUNT, null, null, null);
		return this.conn.getDefaultQueryInterface().executeToTable(sparql).getCellAsInt(0, 0);
	}
	
	
	// TODO move queries to SparqlToXLibUtils
	
	private String importClause() {
		return "prefix rdfs:<http://www.w3.org/2000/01/rdf-schema#> \n";
	}
	
	private String sameAsClause(String var) {
		String typeName = var + "_type";
		return    "    " + typeName + " rdfs:subClassOf* <" + this.sameAsTypeUri + "> . " + var + " a " + typeName + " . \n";
	}
	
	private String targetClause(String subject, String object) {
		String propName = subject + "_" + object.substring(1) +  "_prop";
		return    "    " + propName + " rdfs:subPropertyOf* <" + this.targetPropUri + "> . " + subject + " " + propName + " " + object + ". \n";
	}
	
	private String duplicateClause(String subject, String object) {
		String propName = subject + "_" + object.substring(1) +  "_prop";
		return    "    " + propName + " rdfs:subPropertyOf* <" + this.duplicatePropUri + "> . " + subject + " " + propName + " " + object + ". \n";
	}
	
	/**
	 * Get SameAs that can be processed in next batch because duplicate has no other incoming
	 * Current SPARQL generation and nodegroups are not powerful enough to do this because:
	 * 	1. (issue #405) there is no FILTER NOT EXISTS 
	 *  2. (issue #406) Examples such as RACK SAME_AS with range of type THING generates very long VALUES clauses for types
	 *  3. (          ) Can't group by type
	 * @return
	 * @throws Exception
	 */
	private String generateGetNextSameAsBatchSparql(int limit) throws Exception {

		String query =  
				this.importClause() +
				"select distinct ?same_as ?target ?duplicate (GROUP_CONCAT (?target_type ) as ?target_types)  (GROUP_CONCAT (?duplicate_type ) as ?duplicate_types)\n"
				+ SparqlToXLibUtil.generateSparqlFromOrUsing("", "FROM", this.conn, this.oInfo) + "\n"
				+ " where {\n"
				+ "     " + this.sameAsClause("?same_as")
				+ "     " + this.targetClause("?same_as", "?target") 
				+ "     " + this.duplicateClause("?same_as", "?duplicate") 
				+ "     filter not exists {\n"
				+ "         " + this.targetClause("?other1", "?duplicate")
				+ "	        FILTER (?other1 != ?same_as) . \n"
				+ "	    }\n"
 				+ "     filter not exists {\n"
				+ "         " + this.duplicateClause("?other2", "?duplicate")
				+ "		    FILTER (?other2 != ?same_as) . \n"
				+ "	    }\n"
				+ "     ?target a ?target_type. \n"
				+ "     ?duplicate a ?duplicate_type. \n"
				+ "}\n"
				+ "GROUP BY ?same_as ?target ?duplicate \n" 
				+ "LIMIT " + String.valueOf(limit);
			
		return query;
	}
	
	private String generateGetAllSameAsSparql(int limit) throws Exception {

		String query =  
				this.importClause()
				+ "select distinct ?same_as ?target ?duplicate\n"
				+ SparqlToXLibUtil.generateSparqlFromOrUsing("", "FROM", this.conn, this.oInfo) + "\n"
				+ " where {\n"
				+ "     " + this.targetClause("?same_as", "?target") 
				+ "     " + this.duplicateClause("?same_as", "?duplicate") 
				+ "     " + this.sameAsClause("?same_as")
				+ "}\n";
				
		return query;
	}
	
	private String generateGetCombiningDiffTypes(int limit, int offset) throws Exception {

		String query = 
				this.importClause()
				+ "select distinct ?same_as ?target ?duplicate\n"
				+ SparqlToXLibUtil.generateSparqlFromOrUsing("", "FROM", this.conn, this.oInfo) + "\n"
				+ " where {\n"
				+ "     " + this.targetClause("?same_as", "?target")
				+ "     " + this.duplicateClause("?same_as", "?duplicate")
				+ "     ?target a ?t1. \n"
				+ "     filter not exists { ?duplicate a ?t1 } . \n"
				+ "     " + this.sameAsClause("?same_as")
				+ "}\n"
				+ "limit " + String.valueOf(limit) + " \n"
				+ "offset " + String.valueOf(offset) + " \n"
				;
		
				
		return query;
	}
	
	/** 
	 * two different sameAs have the same dupicate
	 * @return
	 * @throws Exception
	 */
	private String generateGetBadSameAsDuplicates(int limit) throws Exception {

		String query = 
				this.importClause()
				+ "select distinct ?same_as1 ?same_as2 ?duplicate\n"
				+ SparqlToXLibUtil.generateSparqlFromOrUsing("", "FROM", this.conn, this.oInfo) + "\n"
				+ " where {\n"
				+ "     " + this.duplicateClause("?same_as1", "?duplicate")
				+ "     " + this.duplicateClause("?same_as2", "?duplicate")
				+ "     filter (?same_as2 != ?same_as1).\n"
				+ "     " + this.sameAsClause("?same_as1")
				+ "     " + this.sameAsClause("?same_as2")
				+ "}\n"
				;
				
		return query;
	}
	
	/**
	 * Target and duplicate point to same instance
	 * @return
	 * @throws Exception
	 */
	private String generateGetBadSameAsDupEqTarget(int limit) throws Exception {

		String query = 
				this.importClause()
				+ "select distinct ?same_as1 ?object\n"
				+ SparqlToXLibUtil.generateSparqlFromOrUsing("", "FROM", this.conn, this.oInfo) + "\n"
				+ " where {\n"
				// can't use targetClause() for this outlier case where objects are equal
				+ "     ?same_as_target_prop rdfs:subPropertyOf*    <http://research.ge.com/semtk/EntityResolution#target> .    ?same_as ?same_as_target_prop    ?object. \n"
				+ "     ?same_as_duplicate_prop rdfs:subPropertyOf* <http://research.ge.com/semtk/EntityResolution#duplicate> . ?same_as ?same_as_duplicate_prop ?object. \n"
				+ "     " + this.sameAsClause("?same_as")
				+ "}\n"
				;
				
		return query;
	}
	
	private String generateGetBadSameCardinalityErr(int limit) throws Exception {

		String query = 
				this.importClause()
				+ "select distinct ?same_as \n"
				+ SparqlToXLibUtil.generateSparqlFromOrUsing("", "FROM", this.conn, this.oInfo) + "\n"
				+ " where {\n"
				+ "     { \n"
				+ "         " + this.targetClause("?same_as", "?object1")
				+ "         " + this.targetClause("?same_as", "?object2")
				+ "         FILTER (?object1 != ?object2) ."
				+ "         " + this.sameAsClause("?same_as")
				+ "     } UNION { \n"
				+ "         " + this.duplicateClause("?same_as", "?object1")
				+ "         " + this.duplicateClause("?same_as", "?object2")
				+ "         FILTER (?object1 != ?object2) ."
				+ "         " + this.sameAsClause("?same_as")
				+ "     } UNION { \n"
				+ "         " + this.sameAsClause("?same_as")
				+ "         FILTER NOT EXISTS { " + this.targetClause("?same_as", "?object")+ " } .\n"
				+ "     } UNION { \n"
				+ "         " + this.sameAsClause("?same_as")
				+ "         FILTER NOT EXISTS { " + this.duplicateClause("?same_as", "?object")+ " } .\n"
				+ "     }\n"
				+ "}\n"
				;
				
		return query;
	}

}
